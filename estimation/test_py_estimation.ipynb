{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "df_reviews = pd.read_csv('../data/movie_reviews.csv').dropna()\n",
    "df_reviews_pr = pd.read_csv('../data/movie_reviews_processed.csv').dropna()\n",
    "model_reviews = sm.load('../linear_models/data/model_reviews.pickle') # pkl later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_happiness = (\n",
    "    pd.read_csv('../data/world_happiness_2018.csv')\n",
    "    .dropna()\n",
    "    .rename(\n",
    "        columns = {\n",
    "            'happiness_score': 'happiness',\n",
    "            'healthy_life_expectancy_at_birth': 'life_exp',\n",
    "            'log_gdp_per_capita': 'log_gdp_pc',\n",
    "            'perceptions_of_corruption': 'corrupt'\n",
    "        }\n",
    "    )\n",
    "    .assign(\n",
    "        gdp_pc = lambda x: np.exp(x['log_gdp_pc']),\n",
    "    )    \n",
    "    [['country', 'happiness','life_exp', 'gdp_pc', 'corrupt']]\n",
    ")\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df_happiness[['life_exp_sc', 'gdp_pc_sc', 'corrupt_sc']] = scaler.fit_transform(\n",
    "    df_happiness[['life_exp', 'gdp_pc', 'corrupt']]\n",
    ")\n",
    "df_happiness = df_happiness.drop(columns = ['life_exp', 'gdp_pc', 'corrupt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b0</th>\n",
       "      <th>b1</th>\n",
       "      <th>objective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>5.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.490789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      b0   b1  objective\n",
       "899  5.4  0.9   0.490789"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ols(par, X, y, sum = False):\n",
    "    # add a column of 1s for the intercept\n",
    "    X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "    # Calculate the predicted values\n",
    "    y_hat = X @ par\n",
    "    \n",
    "    # Calculate the error\n",
    "    value = np.sum((y - y_hat)**2)\n",
    "    \n",
    "    # Calculate the value as sum or average\n",
    "    if not sum:\n",
    "        value = value / X.shape[0]\n",
    "    \n",
    "    # Return the value\n",
    "    return(value)\n",
    "\n",
    "# create a grid of guesses\n",
    "from itertools import product\n",
    "\n",
    "guesses = pd.DataFrame(\n",
    "    product(\n",
    "        np.arange(1, 7, 0.1),\n",
    "        np.arange(-1, 1, 0.1)\n",
    "    ),\n",
    "    columns = ['b0', 'b1']\n",
    ")\n",
    "\n",
    "# Example for one guess\n",
    "ols(\n",
    "    par = guesses.iloc[0,:],\n",
    "    X = df_happiness['life_exp_sc'],\n",
    "    y = df_happiness['happiness']\n",
    ")\n",
    "\n",
    "# Calculate the function value for each guess\n",
    "guesses['objective'] = guesses.apply(\n",
    "    lambda x: ols(par = x, X = df_happiness['life_exp_sc'], y = df_happiness['happiness']),\n",
    "    axis = 1\n",
    ")\n",
    "\n",
    "min_loss = guesses[guesses['objective'] == guesses['objective'].min()]\n",
    "\n",
    "min_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: Optimization terminated successfully.\n",
       "  success: True\n",
       "   status: 0\n",
       "      fun: 0.48851727833540676\n",
       "        x: [ 5.445e+00  8.838e-01]\n",
       "      nit: 3\n",
       "      jac: [-9.313e-08  7.004e-07]\n",
       " hess_inv: [[ 5.190e-01 -9.564e-02]\n",
       "            [-9.564e-02  9.810e-01]]\n",
       "     nfev: 12\n",
       "     njev: 4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "our_result = minimize(\n",
    "    fun    = ols,\n",
    "    x0     = np.array([1., 0.]),\n",
    "    args   = (np.array(df_happiness['life_exp_sc']), np.array(df_happiness['happiness'])),\n",
    "    method = 'BFGS' # optimization algorithm\n",
    ")\n",
    "\n",
    "\n",
    "our_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10798193, 0.22184167])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# two example life expectancy scores, mean and 1 sd above\n",
    "life_expectancy = np.array([0, 1])\n",
    "\n",
    "# observed happiness scores\n",
    "happiness = np.array([4, 5.2])\n",
    "\n",
    "# predicted happiness with rounded coefs\n",
    "mu = 5 + 1 * life_expectancy\n",
    "\n",
    "# just a guess for sigma\n",
    "sigma = .5\n",
    "\n",
    "# likelihood for each observation\n",
    "L = norm.pdf(happiness, loc = mu, scale = sigma)\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MaxLike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(par, X, y):\n",
    "    # add a column of 1s for the intercept\n",
    "    X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "    # setup\n",
    "    beta   = par[1:]       # coefficients\n",
    "    sigma  = np.exp(par[0])        # error sd\n",
    "\n",
    "    N = X.shape[0]\n",
    "\n",
    "    LP = X @ beta          # linear predictor\n",
    "    mu = LP                # identity link in the glm sense\n",
    "\n",
    "    # calculate (log) likelihood\n",
    "    ll = norm.logpdf(y, loc = mu, scale = sigma) \n",
    "    return(-np.sum(ll))\n",
    "\n",
    "our_result = minimize(\n",
    "    fun    = likelihood,\n",
    "    x0     = np.array([1, 0, 0]),\n",
    "    args   = (np.array(df_happiness['life_exp_sc']), np.array(df_happiness['happiness'])),\n",
    "    # method = \"Nelder-Mead\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: Optimization terminated successfully.\n",
       "  success: True\n",
       "   status: 0\n",
       "      fun: 118.80381119428435\n",
       "        x: [-3.582e-01  5.445e+00  8.838e-01]\n",
       "      nit: 14\n",
       "      jac: [ 1.907e-06  9.537e-07  9.537e-07]\n",
       " hess_inv: [[ 4.555e-03 -1.485e-05 -8.744e-05]\n",
       "            [-1.485e-05  4.411e-03  1.340e-04]\n",
       "            [-8.744e-05  1.340e-04  4.760e-03]]\n",
       "     nfev: 88\n",
       "     njev: 22"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge(par, X, y, lambda_ = 0):\n",
    "    # add a column of 1s for the intercept\n",
    "    X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "    # Calculate the predicted values\n",
    "    mu = X @ par\n",
    "    \n",
    "    # Calculate the error\n",
    "    value = np.sum((y - mu)**2)\n",
    "    \n",
    "    # Add the penalty\n",
    "    value = value + lambda_ * np.sum(par**2)\n",
    "    \n",
    "    return(value)\n",
    "\n",
    "ridge(\n",
    "    X = df_happiness['life_exp_sc'],\n",
    "    y = df_happiness['happiness'],\n",
    "    par = np.array([0, 0]),\n",
    "    lambda_ = 0.1\n",
    ")\n",
    "\n",
    "our_result = minimize(\n",
    "    fun  = ridge,\n",
    "    x0   = np.array([0, 0, 0, 0]),\n",
    "    args = (\n",
    "        np.array(df_happiness.drop(columns=['happiness', 'country'])),\n",
    "        np.array(df_happiness['happiness']), \n",
    "        0.1\n",
    "    ),\n",
    "    # method = 'BFGS',\n",
    "    tol=1e-10,\n",
    "    options={'maxiter': 10000}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.43997501,  0.52188498,  0.43554025, -0.10484642])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_result['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.4448321376528055, array([ 0.52188497,  0.43554027, -0.10484641]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(alpha = 0.1)\n",
    "\n",
    "ridge.fit(\n",
    "    X = df_happiness.drop(columns=['happiness', 'country']),\n",
    "    y = df_happiness['happiness']\n",
    ")\n",
    "\n",
    "ridge.intercept_, ridge.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(par, X, y):\n",
    "    # add a column of 1s for the intercept\n",
    "    X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "    # Calculate the predicted values\n",
    "    y_hat = X @ par\n",
    "    \n",
    "    # Convert to a probability ('sigmoid' function)\n",
    "    y_hat = 1 / (1 + np.exp(-y_hat))\n",
    "    \n",
    "    # likelihood\n",
    "    ll = y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat)\n",
    "    \n",
    "    return(-np.sum(ll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "df_happiness_bin = df_happiness.copy()\n",
    "df_happiness_bin['happiness'] = np.where(df_happiness['happiness'] > 5.5, 1, 0)\n",
    "\n",
    "mod_logloss = minimize(\n",
    "    objective,\n",
    "    x0 = np.array([0, 0, 0, 0]),\n",
    "    args = (\n",
    "        df_happiness_bin[['life_exp_sc', 'gdp_pc_sc', 'corrupt_sc']],\n",
    "        df_happiness_bin['happiness']\n",
    "    )\n",
    ")\n",
    "\n",
    "mod_glm = smf.glm(\n",
    "    'happiness ~ life_exp_sc + corrupt_sc + gdp_pc_sc',\n",
    "    data   = df_happiness_bin,\n",
    "    family = sm.families.Binomial()\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(  message: Optimization terminated successfully.\n",
       "   success: True\n",
       "    status: 0\n",
       "       fun: 40.663473928254746\n",
       "         x: [-1.637e-01  1.809e+00  1.126e+00 -4.627e-01]\n",
       "       nit: 14\n",
       "       jac: [ 9.060e-06  1.907e-06  3.338e-06  9.060e-06]\n",
       "  hess_inv: [[ 1.315e-01 -1.202e-01  1.242e-01 -5.169e-02]\n",
       "             [-1.202e-01  4.133e-01 -2.214e-01 -3.464e-02]\n",
       "             [ 1.242e-01 -2.214e-01  3.781e-01 -2.298e-02]\n",
       "             [-5.169e-02 -3.464e-02 -2.298e-02  1.463e-01]]\n",
       "      nfev: 85\n",
       "      njev: 17,\n",
       " <class 'statsmodels.iolib.summary.Summary'>\n",
       " \"\"\"\n",
       "                  Generalized Linear Model Regression Results                  \n",
       " ==============================================================================\n",
       " Dep. Variable:              happiness   No. Observations:                  112\n",
       " Model:                            GLM   Df Residuals:                      108\n",
       " Model Family:                Binomial   Df Model:                            3\n",
       " Link Function:                  Logit   Scale:                          1.0000\n",
       " Method:                         lbfgs   Log-Likelihood:                -40.663\n",
       " Date:                Sun, 24 Mar 2024   Deviance:                       81.327\n",
       " Time:                        14:56:02   Pearson chi2:                     75.0\n",
       " No. Iterations:                     6   Pseudo R-squ. (CS):             0.4806\n",
       " Covariance Type:            nonrobust                                         \n",
       " ===============================================================================\n",
       "                   coef    std err          z      P>|z|      [0.025      0.975]\n",
       " -------------------------------------------------------------------------------\n",
       " Intercept      -0.1636      0.376     -0.435      0.663      -0.901       0.573\n",
       " life_exp_sc     1.8091      0.666      2.717      0.007       0.504       3.114\n",
       " corrupt_sc     -0.4628      0.403     -1.149      0.251      -1.252       0.327\n",
       " gdp_pc_sc       1.1261      0.655      1.718      0.086      -0.159       2.411\n",
       " ===============================================================================\n",
       " \"\"\")"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_logloss, mod_glm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(\n",
    "    par, \n",
    "    X, \n",
    "    y, \n",
    "    tolerance = 1e-3, \n",
    "    maxit = 1000, \n",
    "    learning_rate = 1e-3, \n",
    "    adapt = False, \n",
    "    verbose = True, \n",
    "    plotLoss = True\n",
    "):\n",
    "    # add a column of 1s for the intercept\n",
    "    X = np.c_[np.ones(X.shape[0]), X]\n",
    "    \n",
    "    # initialize\n",
    "    beta = par\n",
    "    loss = np.sum((X @ beta - y)**2)\n",
    "    tol = 1\n",
    "    iter = 1\n",
    "\n",
    "    while (tol > tolerance and iter < maxit):\n",
    "        LP = X @ beta\n",
    "        grad = X.T @ (LP - y)\n",
    "        betaCurrent = beta - learning_rate * grad\n",
    "        tol = np.max(np.abs(betaCurrent - beta))\n",
    "        beta = betaCurrent\n",
    "        loss = np.append(loss, np.sum((LP - y)**2))\n",
    "        iter = iter + 1\n",
    "\n",
    "        if (adapt):\n",
    "            stepsize = np.where(loss[iter] < loss[iter - 1], stepsize * 1.2, stepsize * .8)\n",
    "\n",
    "        if (verbose and iter % 10 == 0):\n",
    "            print(\"Iteration:\", iter)\n",
    "\n",
    "    if (plotLoss):\n",
    "        plt.plot(loss)\n",
    "        plt.show()\n",
    "\n",
    "    return({\n",
    "        \"par\": beta,\n",
    "        \"loss\": loss,\n",
    "        \"RSE\": np.sqrt(np.sum((LP - y)**2) / (X.shape[0] - X.shape[1])),\n",
    "        \"iter\": iter,\n",
    "        \"fitted\": LP\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_result = gradient_descent(\n",
    "    par = np.array([0, 0, 0, 0]),\n",
    "    X = df_happiness[['life_exp', 'gdp_pc', 'corrupt']].to_numpy(),\n",
    "    y = df_happiness['happiness'].to_numpy(),\n",
    "    learning_rate = 1e-3,\n",
    "    verbose = False,\n",
    "    plotLoss = False # will show below\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'par': array([ 5.43691264,  0.51898243,  0.4370022 , -0.10687814]),\n",
       " 'loss': array([3462.57618111, 3462.57618111, 2719.61584058, 2141.99503995,\n",
       "        1691.29344167, 1338.66327157, 1062.19911351,  845.11737579,\n",
       "         674.46819375,  540.20549918,  434.50363582,  351.24751107,\n",
       "         285.64744415,  233.94527786,  193.18834601,  161.05455731,\n",
       "         135.71639644,  115.7348042 ,   99.97614844,   87.54712969,\n",
       "          77.74367018,   70.01073704,   63.91073402,   59.09861886,\n",
       "          55.30230601,   52.30722677,   49.94416173,   48.07965019,\n",
       "          46.60842954,   45.44747447,   44.53129693,   43.80824002,\n",
       "          43.23755541,   42.78709868,   42.43151162,   42.15078886,\n",
       "          41.92914723,   41.75413416,   41.61592436,   41.50676513,\n",
       "          41.42053882,   41.35241773,   41.29859186,   41.25605417,\n",
       "          41.22243119,   41.19584938,   41.17482968,   41.15820437,\n",
       "          41.14505143,   41.13464277,   41.12640334,   41.11987896,\n",
       "          41.11471081,   41.11061542,   41.10736874,   41.10479374]),\n",
       " 'RSE': 0.6169278256199849,\n",
       " 'iter': 56,\n",
       " 'fitted': array([3.98125875, 5.48315358, 5.66397783, 5.39663751, 6.95938284,\n",
       "        6.86933173, 5.52556616, 5.05028034, 5.52289847, 6.6546165 ,\n",
       "        4.2230131 , 5.03085378, 5.40376724, 4.88237754, 5.43339571,\n",
       "        5.48546359, 4.17700135, 4.0535034 , 6.95631002, 3.73956931,\n",
       "        5.90177729, 5.46405166, 4.56799885, 5.8378002 , 5.86188832,\n",
       "        6.15928423, 7.05327889, 5.41330826, 5.46811592, 5.20729529,\n",
       "        6.08671237, 4.5058061 , 6.85675007, 6.71887712, 4.9495875 ,\n",
       "        5.19515698, 6.81161705, 6.10687203, 5.14545824, 4.2107574 ,\n",
       "        4.3330877 , 5.23510751, 4.74789488, 4.98356156, 5.57344556,\n",
       "        7.50887651, 6.43879884, 6.44207926, 3.87244108, 6.7580625 ,\n",
       "        5.52056123, 4.63241885, 4.91058452, 5.74861994, 5.38730563,\n",
       "        4.25647137, 5.86667714, 8.10745479, 5.4142672 , 4.49007543,\n",
       "        4.43788594, 5.74379284, 3.99624062, 4.48654493, 5.59245327,\n",
       "        5.63492884, 5.01455258, 5.02550875, 5.65724436, 4.22263305,\n",
       "        4.73223108, 4.51546331, 4.9376965 , 6.98333045, 6.82145775,\n",
       "        5.33202438, 4.19532406, 3.88288843, 7.50089153, 4.58975159,\n",
       "        5.85770826, 5.44007227, 4.95878423, 6.18212446, 5.66334203,\n",
       "        5.48693368, 5.07095626, 4.60428266, 5.52242485, 3.87626105,\n",
       "        5.96499918, 6.20893469, 4.58650323, 6.5221634 , 6.55389856,\n",
       "        5.38883192, 7.01734488, 7.38265694, 4.56038691, 5.47990699,\n",
       "        4.20132552, 5.34885248, 5.72252902, 4.25698313, 5.04551569,\n",
       "        6.72960187, 6.63060502, 5.85103086, 5.28314658, 5.31643152,\n",
       "        4.30195557, 4.26046193])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x6/4jhswqxj0sqf_gkgq6lw6l880000gn/T/ipykernel_11473/2417730363.py:55: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  fits[i] = LP\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 5.42766433,  0.53158177,  0.39770852, -0.14934094])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stochastic_gradient_descent(\n",
    "    par, # parameter estimates\n",
    "    X,   # model matrix\n",
    "    y,   # target variable\n",
    "    learning_rate = 1, # the learning rate\n",
    "    stepsize_tau = 0,  # if > 0, a check on the LR at early iterations\n",
    "    average = False    # a variation of the approach\n",
    "):\n",
    "    # initialize\n",
    "    np.random.seed(1234)\n",
    "\n",
    "    # shuffle the data\n",
    "    idx = np.random.choice(\n",
    "        df_happiness.shape[0], \n",
    "        df_happiness.shape[0], \n",
    "        replace = False\n",
    "    )\n",
    "    X = X[idx, :]\n",
    "    y = y[idx]\n",
    "    \n",
    "    X = np.c_[np.ones(X.shape[0]), X]\n",
    "    beta = par\n",
    "\n",
    "    # Collect all estimates\n",
    "    betamat = np.zeros((X.shape[0], beta.shape[0]))\n",
    "\n",
    "    # Collect fitted values at each point))\n",
    "    fits = np.zeros(X.shape[0])\n",
    "\n",
    "    # Collect loss at each point\n",
    "    loss = np.zeros(X.shape[0])\n",
    "\n",
    "    # adagrad per parameter learning rate adjustment\n",
    "    s = 0\n",
    "\n",
    "    # a smoothing term to avoid division by zero\n",
    "    eps = 1e-8\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        Xi = X[None, i, :]\n",
    "        yi = y[i]\n",
    "\n",
    "        # matrix operations not necessary here,\n",
    "        # but makes consistent with previous gd func\n",
    "        LP = Xi @ beta\n",
    "        grad = Xi.T @ (LP - yi)\n",
    "        s = s + grad**2 # adagrad approach\n",
    "\n",
    "        # update\n",
    "        beta = beta - learning_rate / \\\n",
    "            (stepsize_tau + np.sqrt(s + eps)) * grad\n",
    "\n",
    "        betamat[i, :] = beta\n",
    "\n",
    "        fits[i] = LP\n",
    "        loss[i] = np.sum((LP - yi)**2)\n",
    "\n",
    "    LP = X @ beta\n",
    "    lastloss = np.sum((LP - y)**2)\n",
    "\n",
    "    return({\n",
    "        \"par\": beta, # final estimates\n",
    "        \"par_chain\": betamat, # estimates at each iteration\n",
    "        \"MSE\": lastloss / X.shape[0],\n",
    "        \"fitted\": LP\n",
    "    })\n",
    "X_train = df_happiness[['life_exp_sc', 'gdp_pc_sc', 'corrupt_sc']]\n",
    "y_train = df_happiness['happiness']\n",
    "\n",
    "our_result = stochastic_gradient_descent(\n",
    "    par = np.array([np.mean(df_happiness['happiness']), 0, 0, 0]),\n",
    "    X = X_train.to_numpy(),\n",
    "    y = y_train.to_numpy(),\n",
    "    learning_rate = .15,\n",
    "    stepsize_tau = .1\n",
    ")\n",
    "\n",
    "our_result['par']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.45437658,  0.51165618,  0.45536819, -0.10404313])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bootstrap(X, y, nboot=100, seed=123):\n",
    "    # add a column of 1s for the intercept\n",
    "    X = np.c_[np.ones(X.shape[0]), X]\n",
    "    N = X.shape[0]\n",
    "\n",
    "    # initialize\n",
    "    beta = np.empty((nboot, X.shape[1]))\n",
    "    \n",
    "    # beta = pd.DataFrame(beta, columns=['Intercept'] + list(cn))\n",
    "    mse = np.empty(nboot)    \n",
    "\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    for i in range(nboot):\n",
    "        # sample with replacement\n",
    "        idx = np.random.randint(0, N, N)\n",
    "        Xi = X[idx, :]\n",
    "        yi = y[idx]\n",
    "\n",
    "        # estimate model\n",
    "        model = LinearRegression(fit_intercept=False)\n",
    "        mod = model.fit(Xi, yi)\n",
    "\n",
    "        # save results\n",
    "        beta[i, :] = mod.coef_\n",
    "        mse[i] = np.sum((mod.predict(Xi) - yi)**2) / N\n",
    "\n",
    "    # given mean estimates, calculate MSE\n",
    "    y_hat = X @ beta.mean(axis=0)\n",
    "    final_mse = np.sum((y - y_hat)**2) / N\n",
    "\n",
    "    return dict(beta = beta, mse = mse, final_mse = final_mse)\n",
    "\n",
    "our_result = bootstrap(\n",
    "    X = df_happiness[['life_exp_sc', 'gdp_pc_sc', 'corrupt_sc']],\n",
    "    y = df_happiness['happiness'],\n",
    "    nboot = 250\n",
    ")\n",
    "\n",
    "np.mean(our_result['beta'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.46243972,  0.51282588,  0.45531028, -0.09958016])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_result['beta'].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book-of-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
