---
title: "Untitled"
format: html
---


## Mixed

<!-- ### Why Should You Care? {#sec-mixed-models-why}

Structures within your data are important and paying attention to how data might be grouped in some way will let you generate more insights about how observations within and between groups might behave. For example, people working in the same organization, or students in the same school, are likely to have a more similar experiences than those from different organizations or schools. This shared connection within groups may ultimately affect what we see in our chosen outcomes. Mixed models will let us handle grouped data such as this, all while getting the benefits of a standard linear model. -->


<!-- A classic example of such grouping structure is students in a classroom. Those groups can also be nested within larger groups -- students nested within classrooms, nested within schools, nested within districts. But the grouping structure can also capture the notion of a repeated measure for an individual, like repeated test scores.  -->

<!-- While we aren't rejecting the idea of a mean with these mixed-models, we are implying that each group (whether it is a group, nested group, or repeated observations from a person) has its own unique mean that can be useful for modeling the target.  -->



<!-- TODO: Not sure need another example?

Before we go to modeling our reviews, let's consider an example training program to increase vertical jump, with average vertical increases of 2 inches. That really doesn't sound all that impressive; however, that increase came across 5 distinct groups: NBA players, NFL players, NHL players, MLB players, and data analysts. We can be pretty certain that each group has a very different vertical jump distribution coming into this training program. Given the amount of jumping that NBA players do, this program is unlikely to produce dramatic increases in vertical jump for them. We would probably expect modest gains in the NFL and even greater gains within NHL and MLB players. Where we are going to see the best gains, though, is from data analysts -- they might want to jump to conclusions, but don't need to jump over their computers very often. Now that we know the additional information, can we just look at the average increase and be satisfied? Probably not. Instead, we need to look at the group that individuals might be in and judge accordingly. A mixed-model is going to let us to model all of this without too much work. -->


Here is the by-hand Model

<!-- 
### Using a Mixed Model  {#sec-mixed-models-standard}

Now let's fit some mixed models with `lme4` or `statsmodels`. We will also create `null models` (i.e., models with an intercept only), since we will need some information from them later.



:::{.panel-tabset}

##### R
```{r}
library(lme4)

fit_mer = lmer(rating ~ total_reviews_sc + (1 | genre), 
               df_reviews, 
               REML = FALSE)

summary(fit_mer)

null_model = lmer(rating ~ 1 + (1 | genre), 
                   df_reviews, 
                   REML = FALSE)

summary(null_model, correlation = FALSE)
```

##### Python

```{python}
import statsmodels.api as sm

fit_mer = sm.MixedLM.from_formula("rating ~ total_reviews_sc", df_reviews, 
                                  groups=df_reviews["genre"])

fit_mer = fit_mer.fit()

fit_mer.summary()

null_model = sm.MixedLM.from_formula("rating ~ 1", df_reviews, 
                                     groups=df_reviews["genre"])
```

:::



What can we take from these summaries, and what are we getting beyond the linear model? 

For starters, we should notice a change in our standard errors -- by integrating information about the groups, we are getting a better sense of how much uncertainty our model contains at the global average level.

We also see some additional information for our random effects. The standard deviation is telling us how much the rating moves around based upon genre after getting the information from our fixed effects (i.e., the rating can move around nearly .3 points from genre alone). 

We can use information for the random effects components to get some additional information. For instance, we can calculate the proportion of variance in scores that is accounted for by the genre alone:

$$\frac{\text{intercept variance}}{\text{intercept variance} + \text{residual variance}}$$

With our values, we have:

$$
.08309 / (.08309 + .25482) = 0.2458939
$$

This is what is known as the intraclass correlation (ICC). It ranges from 0 (no variance between clusters) to 1 (variance between clusters). With an ICC of .25, we can say that 25% of the variance in scores is accounted for by the genre alone.


TODO: WEED PULLING

We can also use various bits of information in our output to create different *R^2* values. 

For the first level of the model, we can calculate the $R^2$ as:

$$R^2_1 - \frac{\sigma^2_{M1} + \tau^2_{M1}}{\sigma^2_{M0} + \tau^2_{M0}}$$

We can pull that information from our two model outputs:

```{r}
m1Sigma = .08309 # full model random effect intercept

m1Tau = .25482 # full model random effect residual

m0Sigma = .07557 # null model random effect intercept

m0Tau = .31371 # null model random effect residual

1 - ((m1Sigma + m1Tau) / (m0Sigma + m0Tau))
```

The fixed effect of number of reviews accounts for nearly 13% of the variation within the reviews.

The second level can be calculated as:

$$R^2_2 = 1 - \frac{\sigma^2_{M1} / B + \tau^2_{M1}}{\sigma^2_{M0} / B + \tau^2_{M0}}$$

We see that we added a *B* into the mix here and it is just the average size of the level 2 units (1000 observations / 8 genres). 

```{r}
level2Mean = 1000 / 8

r2Numerator = m1Sigma / level2Mean + m1Tau

r2Denominator = m0Sigma / level2Mean + m0Tau

1 - (r2Numerator / r2Denominator)
```

Which gives us .18 or 18% of the variation in scores is accounted for by the genre alone.

You can also get these values in R like this:

```{r}
performance::r2(fit_mer)
MuMIn::r.squaredGLMM(fit_mer)
```

Here we have two values: the marginal R2 (R2m) and the conditional R2 (R2c). You can think of the marginal values as the standard type of R2 -- it is the variability explained by the fixed effects part of the model (it is what we have already done above). The conditional R2 is using both fixed and random effects, so you can think of it as the total variability explained. Clearly, we can subtract the two to get a better understanding of the effect of the random effects. With $.362 - .154 = .208$, we can say that the random effects account for 20.8% of the variation in ratings. 

Notice that those values are a bit different than what we produced by hand. Packages are now using a revised method proposed by Nakagawa, Johnson, and Schielzeth (2017) that is a bit more accurate than the previous method.

We can also get a good sense of the random effect estimates:

```{r}
#| echo: false
#| label: fig-random-effects-2
#| fig-cap: Random effects estimates
library(sjPlot)

plot_model(fit_mer, type = "re") + 
  theme_clean()
```

For @fig-random_effects, the easiest way to think about it is that the values are effects for each individual random effect (i.e., each genre's random intercept). Since we are just dealing with intercepts right now, they are the deviations from the fixed intercept. The intercept for the model is ~2.6 and the random effect for `comedy` is .48. If we wanted to predict scores for a comedy movie, we would take 2.6 + .48 for the intercept portion of the model (the same would go for any random slopes in the model). In the end, it is showing how much the intercept shifts from genre to genre, and some genres have a positive effect beyond the average and others have a negative effect (`sci-fi`, for instance, is well below the global average).

Remember your group-by and summarize task earlier? Each point is the difference between a genre's average and the overall average -- values in blue are higher than the average and values in red are lower than the average. With that, the average rating for `comedy` is much better than the global average rating, while `sci-fi` is much worse than the global average rating.

### Model Matrix

Let's start our homebrewing adventures by creating a model matrix. We are going to use the `model.matrix()` function to create our model matrix. We are going to use the `user_age` variable as our fixed effect and `genre` as our random effect. We are going to use the `factor()` function to make sure that `genre` is treated as a categorical variable. We are also going to use the `-1` to remove the intercept from the model matrix. 

:::{.panel-tabset}

##### R

```{r}
#| echo: true
#| label: mixed_model_prep

X = model.matrix(~total_reviews_sc, df_reviews)
Z = model.matrix(~factor(df_reviews$genre) - 1)

colnames(Z) = paste0("released_", sort(unique(df_reviews$genre)))

y = df_reviews$rating
```

##### Python

```{python}
#| echo: true
#| label: py_mixed_model_prep

X = df_reviews[['total_reviews_sc']]
X = sm.add_constant(X)
X = X.to_numpy()

Z = pd.get_dummies(df_reviews['genre'], drop_first=True)
Z = Z.to_numpy()

y = df_reviews['rating']
y = y.to_numpy()
```

:::

### Likelihood Function

:::{.panel-tabset}

##### R

```{r, r_data_read}
#| echo: true
#| label: mixed_model_ll

mixed_log_likelihood = function(y, X, Z, theta) {
  tau   = exp(theta[1])
  sigma = exp(theta[2])
  n = length(y)
  
  # evaluate covariance matrix for y
  e  = tcrossprod(Z)*tau^2 + diag(n)*sigma^2
  b  = coef(lm.fit(X, y))
  mu = X %*% b

  ll = mvtnorm::dmvnorm(y, mu, e, log = TRUE)
  -ll
}
```

##### Python

```{python}
#| echo: true
#| label: py_mixed_model_ll
from scipy import stats

def mixed_log_likelihood(theta, y, X, Z):
    tau = np.exp(theta[0])
    sigma = np.exp(theta[1])
    n = len(y)
    
    e = (Z.dot(Z.T) * tau**2) + (np.eye(n) * sigma**2)
    b = np.linalg.lstsq(X, y, rcond=None)[0]
    mu = X.dot(b) 
    
    ll = stats.multivariate_normal.logpdf(y, mu, e)
    return -ll
```

:::

### Model Fitting

:::{.panel-tabset}

##### R

```{r}
#| label: r-mixed_model_fit
param_init = c(0, 0)

names(param_init) = c('tau', 'sigma')

fit = optim(
  fn  = mixed_log_likelihood,
  X   = X,
  y   = y,
  Z   = Z,
  par = param_init,
  # control = list(reltol = 1e-10)
)

exp(fit$par) # compare to sd of random effects
```

##### Python

```{python}
#| eval: false
#| label: mixed_model_fit

from scipy import optimize as opt
def mixed_log_likelihood(theta, y, X, Z):
    tau = np.exp(theta[0])
    sigma = np.exp(theta[1])
    n = len(y)
    
    e = (Z.dot(Z.T) * tau**2) + (np.eye(n) * sigma**2)
    b = np.linalg.lstsq(X, y, rcond=None)[0]
    mu = X.dot(b) 
    
    ll = stats.multivariate_normal.logpdf(y, mu, e)
    return -ll
theta = np.array([0, 0])

mixed_log_likelihood(theta, y, X, Z)

fit = opt.minimize(
    fun=mixed_log_likelihood,
    x0=theta,
    args=(y, X, Z),
    # tol=1e-5
)

np.exp(fit.x) # compare to sd of random effects
```

:::

 -->

## GAM


<!-- But what if we want to allow the effect of a feature to vary depending on its own values? This is called a **curvilinear** effect, and we can use a linear model to capture this as well. -->
<!-- 
### Why Should You Care? {#sec-gam-why}

Not every relationship is linear and not every relationship is monotonic. Sometimes, you need to be able to model a relationship that has a fair amount of nonlinearity -- they can appear as slight curves, waves, and any other type of wiggle that you can imagine. Additive models will give you the ability to model those nonlinear relationships between your features and target. -->

<!-- 

TODO: Move to estimation, but may need to nix for pdf brevity, and needs some work to fit well.

### Splines {#sec-gam-splines}

Now that you've gotten a taste of a standard way of specifying a GAM, let's roll our own. We are going to need to generate several functions to make this work. The first will be to produce the *cubic spline*. Do take note that there are many different types of splines that could be used.

:::{.panel-tabset}

##### R

```{r, r_cubic_spline}
#| echo: true
#| label: r_cubic_spline
cubic_spline = function(x, z) {
  ((z - 0.5)^2 - 1/12) * ((x - 0.5)^2 - 1/12)/4 -
    ((abs(x - z) - 0.5)^4 - (abs(x - z) - 0.5)^2 / 2 + 7/240) / 24
}
```

##### Python

```{python, python_cubic_spline}
#| echo: true
#| label: python_cubic_spline
import numpy as np

def cubic_spline(x,z):
    return (((z - 0.5)**2 - 1/12) * ((x - 0.5)**2 - 1/12)/4 -
            ((np.abs(x - z) - 0.5)**4 - (np.abs(x - z) - 0.5)**2 / 2 + 7/240) / 24)
```

:::

### Model Matrix Function {#sec-gam-model-matrix}

Then we a function to produce the model matrix:

:::{.panel-tabset}

##### R

```{r, r_model_matrix}
#| echo: true
#| label: r_model_matrix
splX = function(x, knots) {
  q = length(knots) + 2        # number of parameters
  n = length(x)                # number of observations
  X = matrix(1, n, q)          # initialized model matrix
  X[ ,2] = x                   # set second column to x
  X[ ,3:q] = outer(x, knots, FUN = cubic_spline) 
  X
}
```

##### Python

```{python, python_model_matrix}
#| echo: true
#| label: python_model_matrix

def splX(x, knots):
    q = len(knots) + 2
    n = len(x)
    X = np.ones((n, q))
    X[:,1] = x
    for i in range(2, q):
        X[:,i] = cubic_spline(x, knots[i-2])
    return X
```

This model matrix will help us to produce an *unpenalized spline*. 

:::



### Model Matrix {#sec-gam-model-matrix}

We can create a model with 4 knots -- you can think of knots as places where individual regression lines will get joined together. You can always experiment with more or less knots. Once we have our knots ready, we can create the model matrix.

As soon as you create your `X` object, you should take a look at it. It will be a matrix with 4 columns. The first column will be all 1s, the second column will be the scaled `user_age`, and the last two columns will be the cubic splines.

:::{.panel-tabset}

##### R

```{r, r_knots}
#| echo: true
#| label: r_knots
knots = 1:4/5
```

```{r, r_model_matrix_spline}
#| echo: true
#| label: r_model_matrix_spline
rating = df_reviews$rating

x = df_reviews$total_reviews_sc

X = splX(x, knots)            
```

```{r, model_matrix_head}
#| echo: true
#| label: model_matrix_head
head(X)
```

##### Python

```{python, python_knots}
#| echo: true
#| label: python_knots

knots = np.arange(1, 5) / 5
```

```{python, python_model_matrix_spline}
#| echo: true
#| label: python_model_matrix_spline

x = df_reviews['total_reviews_sc']

rating = df_reviews['rating']

X = splX(x, knots)
```

```{python, py_model_matrix_head}
#| echo: true
#| label: py_model_matrix_head

X[:5,:]
```

:::

### Model Fitting {#sec-gam-model-fitting}

Now that we have a model matrix, `X`, we can fit the model. All of the hard work was done in creating the model matrix and we can just use `lm` or `OLS` to fit the model.
  
:::{.panel-tabset}

##### R
```{r, r_cubic_model_fitting}
#| echo: true
#| label: r_cubic_model_fitting
fit_lm = lm(rating ~ X - 1)

fit_lm
```

##### Python

```{python, python_cubic_model_fitting}
#| echo: true
#| label: python_cubic_model_fitting
import statsmodels.api as sm

fit_lm = sm.OLS(rating, X).fit()

fit_lm.summary()
```

:::

### Prediction

We can set some prediction values for this model:

TODO: fix this to reflect total reviews, not 0:1; Don't worry until we know where it goes.

:::{.panel-tabset}

##### R
  
```{r, r_predictions}
#| echo: true
#| label: r_predictions
# xp = seq(0, 1, by = .01)
xp = seq(-3, 3, by = .01)
Xp = splX(xp, knots)  
```

##### Python

```{python, python_predictions} 
#| echo: true

xp = np.arange(0, 1, 0.01)
Xp = splX(xp, knots)
```

:::

While creating those predictions is nice, using them to visualize the model is far more helpful.
  
```{r, spline_viz}
#| echo: false
#| label: fig-spline_viz
#| fig-cap: Visualizing cubic regression spline
ggplot(aes(x = x, y = rating), data = data.frame(x, rating)) +
  geom_point() +
  geom_smooth(method = "gam", se = FALSE, color = "#00AAFF") +
  theme_clean()
```

In @fig-spline_viz, we can see that the relationship starts a bit flat, increases, and then flattens out again. This is a pretty good example of a nonlinear relationship.

### Penalized Cubic Spline {#sec-gam-penalty}

Recall that this is an unpenalized cubic spline. If we want to have a finer degree of control over that wiggly line, we can include a **lambda penalty**. 

We'll need to change up our spline function just a bit.

:::{.panel-tabset}

##### R
  
```{r, r_spline_penalty}
#| echo: true
#| label: r_spline_penalty
splS = function(knots) {
  q = length(knots) + 2
  S = matrix(0, q, q) 
  S[3:q, 3:q] = outer(knots, knots, FUN = cubic_spline)
  S
}
```

##### Python

```{python, python_spline_penalty}
#| echo: true
#| label: python_spline_penalty

def splS(knots):
    q = len(knots) + 2
    S = np.zeros((q, q))
    S[2:, 2:] = cubic_spline(knots, knots[:,None])
    return S

```

:::

We also need to be able to take the square root of our entire matrix. This is a bit more complicated than it sounds. We need to take the eigenvalue decomposition of the matrix, take the square root of the eigenvalues, and then recombine the matrix.

:::{.panel-tabset}

##### R

```{r, r_matrix_square}
#| echo: true
#| label: r_matrix_square
mat_sqrt = function(S) {
  d = eigen(S, symmetric = TRUE)
  rS = d$vectors %*% diag(d$values^.5) %*% t(d$vectors)
  rS
}
```

##### Python

```{python, python_matrix_square}
#| echo: true
#| label: python_matrix_square

def mat_sqrt(S):
    w, v = np.linalg.eig(S)
    rS = v @ np.diag(w**.5) @ v.T
    return rS
```

:::

### Penalized Model Fitting Function {#sec-gam-penalty-model}

With those functions in hand, we can create the function to fit the entire model.

:::{.panel-tabset}

##### R
  
```{r, r_penalized_fit}
#| echo: true
#| eval: false
#| label: r_penalized_fit
prs_fit = function(y, x, knots, lambda) {
  q  = length(knots) + 2    # dimension of basis
  n  = length(x)            # number of observations
  Xa = rbind(splX(x, knots), mat_sqrt(splS(knots))*sqrt(lambda)) # augmented model matrix
  y[(n + 1):(n+q)] = 0      # augment the data vector
  
  lm(y ~ Xa - 1) # fit and return penalized regression spline
}
```

##### Python

```{python} 
#| echo: true
#| eval: false
#| label: python_penalized_fit
def prs_fit(y, x, knots, lambda):
    q = len(knots) + 2
    n = len(x)
    Xa = np.vstack(
      (splX(x, knots), mat_sqrt(splS(knots))*np.sqrt(lambda))
      )
    y_add = np.zeros(q)
    y = y.to_numpy()
    y = np.concatenate((y, y_add), axis = 0)
    return sm.OLS(y, Xa).fit()
```

:::

Notice again that magic happens in the model matrix, but that we are still just using `lm` or `OLS` to fit the model.

### Penalized Model Fitting {#sec-gam-penalty-fitting}

Let's stick with 4 knots and see what happens when we set our lambda to .1:

:::{.panel-tabset}

##### R
  
```{r, r_lambda_1}
#| echo: true
#| label: r_lambda_1
knots = 1:4/5

fit_penalized = prs_fit(
  y = rating,
  x = x,
  knots = knots,
  lambda = .1
) 

Xp = splX(xp, knots) 
```

##### Python

```{python}
#| eval: false # change if used later
#| label: python_lambda_1

knots = np.arange(1, 5)/5

fit_penalized = prs_fit(
    y = y,
    x = x,
    knots = knots,
    lamba = .1
)

Xp = splX(xp, knots)
```

:::

As shown in @fig-r_lambda_1_viz, there is some wiggle to that line, but it is not as extreme as what we saw with our unpenalized cubic spline. 

TODO: fix plot scale (doesn't match raw or scaled data)

```{r}
#| echo: false
#| label: fig-r_lambda_1_viz
#| fig-cap: GAM model with lambda set to .1
ggplot(aes(x = x, y = rating), data = data.frame(x, rating)) +
  geom_point() +
  geom_line(aes(x = xp, y = fit[,1]),
            data = tibble(xp, fit = Xp %*% coef(fit_penalized)),
            color = "#00AAFF") +
  theme_clean()
```

We can test out what happens at different lambda values:
  
```{r, lambda_value_viz}
#| echo: false
#| label: fig-lambda_value_viz
#| fig-cap: GAM model with different lambda values
plot_data = purrr::map_df(c(.9, .5, .1, .01, .001), ~{
  fit_penalized = prs_fit(
    y = rating,
    x = x,
    knots = knots,
    lambda = .x
  ) 
  Xp = splX(xp, knots)
  
  results = data.frame(
    x = xp,
    y = Xp %*% coef(fit_penalized),
    lambda = as.factor(.x)
  )
  
  return(results)
})

ggplot(aes(x = x, y = rating), data = data.frame(x, rating)) +
  geom_point() +
  geom_line(
    aes(x = x, y = y, color = lambda),
    data = plot_data
  ) +
  scale_color_manual(values = okabe_ito[c(-1,-4)]) +
  coord_cartesian(xlim = c(-2, 2)) +
  theme_clean()
```

What can we take from @fig-lambda_value_viz? As lambda values get closer to 1, we see lines that look very similar to a standard linear model. If you recall the our function to fit the model, we multiplied the square root of the matrix by the square root of the lambda value; since the square root of 1 is 1, we wouldn't see anything too interesting happen. As our lambda value gets lower, we see an increasing amount of wiggle happen. 

Naturally, this is a great time to think about how these models would work on new data. As lambda gets smaller, we are fitting our in-sample data much better. How do you think this would fare with unseen data? If you'd say that we would do well with training and horrible on testing, we'd likely agree with you.  

-->





<!-- For the results of our gam model, one of the best places to look first is the `edf`/`EDoF` column or attribute. This column indicates the *effective degrees of freedom*. You can think of it as a measure of wiggle in the relationship between the predictor and the target. The higher the value, the more wiggle you have. If you have a value close to 1, then you have a linear relationship. With our current result, we can be pretty confident that a nonlinear relationship gives a better idea about the relationship between `total_reviews` and `rating` than a linear relationship. -->


<!-- 
Depending on the tool we used, we may also get information about our Adjusted $R^2$ (**R-sq.(adj)**) and **Deviance explained**, which is an analog to the unadjusted $R^2$ value for a Gaussian model. We also have **GCV** -- the generalized cross validation score. It is an estimate of the mean square prediction error based on a leave-one-out cross validation estimation process. Naturally, the lower the GCV, the better the model. -->


## Quantile

<!-- ### Why Should You Care? {#sec-quantile-why}

Sometimes the mean doesn't make as much sense to focus on for summarizing our data, whether due to extreme scores that have too much influence, or you're interested at how your model is performing in different parts of your data, such as the top 10%, quantile regression can be helpful. Quantile regression will give you the ability to model the relationship between your features and target at different quantiles of your target, with the median being a starting point. Maybe people who are older are more likely to rate movies higher, but that relationship is stronger for people who rate movies higher than the median. Quantile regression will let you model that relationship. -->


<!-- 
### Data Import and Preparation {#sec-quantile-data}

TODO: Rescale total_reviews or use the scaled version

:::{.panel-tabset}

##### R

```{r}
#| eval: false
#| label: r_data_prep
df_reviews = read.csv("data/movie_reviews.csv")

df_reviews = na.omit(df_reviews)

X = df_reviews$total_reviews
X = cbind(1, X)
y = df_reviews$rating
```

##### Python

```{python}
#| eval: false
#| label: py_data_prep
import pandas as pd
import numpy as np

df_reviews = pd.read_csv("data/movie_reviews.csv")

df_reviews = df_reviews.dropna()

X = pd.DataFrame(
  {'intercept': 1, 
  'total_reviews': df_reviews['total_reviews']}
)
y = df_reviews['rating']
```
::: 
 -->



Models varied in data and type of model, so removed this
### Performance Comparisons

<!-- TODO: UPDATE WITH CURRENT MODELS -->

Just for giggles, we should see how all of our models perform:

```{r}
#| echo: false
#| label: tbl-model-performance-comp
#| tbl-cap: Comparing model performance with RMSE
library(mgcv)
model_data = data.frame(rating = df_reviews$rating, 
                         total_reviews = df_reviews$total_reviews_sc, 
                         genre = df_reviews$genre)
lm_test = lm(rating ~ total_reviews, 
              data = model_data) 
median_test = rq(rating ~ total_reviews, 
                  data = model_data) 
gam_test = gam(rating ~ 
                  s(total_reviews, bs = "cr", fx = FALSE, m = .001), 
                data = model_data) 

fit_mer = lmer(rating ~ total_reviews + (1 | genre), 
               model_data, 
               REML = FALSE)

gt(
  tibble(model = c("standard", "median", "gam", "mixed"), 
             rmse = c(modelr::rmse(lm_test, model_data),
                      modelr::rmse(median_test, model_data),
                      modelr::rmse(gam_test, model_data), 
                      modelr::rmse(fit_mer, model_data))
  )
)
```

Let's check out the results in @tbl-model-performance-comp. Unsurprisingly, the standard linear model and the median regression were pretty close to each other. GAM offered a small bump in performance, but our best model came from the mixed model. This finding may or may not surprise you -- as you spend more time with models, you often encounter situations where simple models outperform more complex models, or are on par with them. Here, we are seeing that the mixed model is offering us a better fit to the data than the other models. However, that doesn't mean that you can just go right to the mixed model. You need to know your data and know what you are trying to accomplish.