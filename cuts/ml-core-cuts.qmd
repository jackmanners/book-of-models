---
title: "Untitled"
format: html
---


<!-- 
```{r}
#| echo: false
#| label: demo-confusion-matrix
#| tbl-cap: Example Confusion Matrix

cm = as.table(matrix(c(62, 10, 10, 18), nrow = 2, byrow = TRUE))

cm = tibble(
  ` ` = c('Predicted Negative', 'Predicted Positive'),
  `Observed Negative` = c(62, 10),
  `Observed Positive` = c(10, 18)
) 
cm |> 
  gt::gt()
```

The diagonal of the confusion matrix is the number of correct predictions, and the off-diagonal is the number of incorrect predictions. In this particular example we have an accuracy of 80 correct out of 100 total, or 80%. However, there are [many metrics we can calculate from this simple confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix), and many of these can also be extended to the multiclass setting. For example, we can calculate the accuracy in general as well as for each class. For an overview of common metrics, refer to @tbl-performance-metrics.

 -->


<!-- In the validation stage, we usually will separate data into training and validation sets. We will go into details later, but the gist is that we select the model with parameters that result in the best performance on the validation set(s), and then we evaluate that model on a test set that has not been used at all in the modeling process as our final assessment of performance. Although not guaranteed, this is the best way to potentially get an unbiased estimate of the model's performance. We may possibly repeat this process for other models we are considering, and then select the model that performs best on the test set.  -->